{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-05T17:44:25.594897Z",
     "iopub.status.busy": "2025-11-05T17:44:25.594060Z",
     "iopub.status.idle": "2025-11-05T17:44:25.602009Z",
     "shell.execute_reply": "2025-11-05T17:44:25.601313Z",
     "shell.execute_reply.started": "2025-11-05T17:44:25.594863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:44:47.434353Z",
     "iopub.status.busy": "2025-11-05T17:44:47.433599Z",
     "iopub.status.idle": "2025-11-05T17:44:47.438239Z",
     "shell.execute_reply": "2025-11-05T17:44:47.437561Z",
     "shell.execute_reply.started": "2025-11-05T17:44:47.434328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#----------------------------- DON'T CHANGE THIS --------------------------\n",
    "DATA_SEED = 67\n",
    "TRAINING_SEED = 1234\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:44:48.794004Z",
     "iopub.status.busy": "2025-11-05T17:44:48.793729Z",
     "iopub.status.idle": "2025-11-05T17:44:48.798345Z",
     "shell.execute_reply": "2025-11-05T17:44:48.797501Z",
     "shell.execute_reply.started": "2025-11-05T17:44:48.793984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:44:56.744345Z",
     "iopub.status.busy": "2025-11-05T17:44:56.743640Z",
     "iopub.status.idle": "2025-11-05T17:44:56.747552Z",
     "shell.execute_reply": "2025-11-05T17:44:56.747011Z",
     "shell.execute_reply.started": "2025-11-05T17:44:56.744319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:44:57.993828Z",
     "iopub.status.busy": "2025-11-05T17:44:57.993574Z",
     "iopub.status.idle": "2025-11-05T17:44:58.005331Z",
     "shell.execute_reply": "2025-11-05T17:44:58.004728Z",
     "shell.execute_reply.started": "2025-11-05T17:44:57.993809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random.seed(DATA_SEED)\n",
    "np.random.seed(DATA_SEED)\n",
    "torch.manual_seed(DATA_SEED)\n",
    "torch.cuda.manual_seed(DATA_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:44:59.714286Z",
     "iopub.status.busy": "2025-11-05T17:44:59.713715Z",
     "iopub.status.idle": "2025-11-05T17:44:59.770082Z",
     "shell.execute_reply": "2025-11-05T17:44:59.769525Z",
     "shell.execute_reply.started": "2025-11-05T17:44:59.714264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path= '../Data/train.csv'     # enter your data path here\n",
    "df = pd.read_csv(data_path)       # read it and store it in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:01.134311Z",
     "iopub.status.busy": "2025-11-05T17:45:01.133960Z",
     "iopub.status.idle": "2025-11-05T17:45:01.150400Z",
     "shell.execute_reply": "2025-11-05T17:45:01.149778Z",
     "shell.execute_reply.started": "2025-11-05T17:45:01.134291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split train df into train_df(80%) and test_df (20%) use seed\n",
    "# ------------------- write your code here -------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=DATA_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:04.334279Z",
     "iopub.status.busy": "2025-11-05T17:45:04.333730Z",
     "iopub.status.idle": "2025-11-05T17:45:04.338010Z",
     "shell.execute_reply": "2025-11-05T17:45:04.337295Z",
     "shell.execute_reply.started": "2025-11-05T17:45:04.334257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create a simple space-based tokenizer.\n",
    "# ------------------- write your code here -------------------------------\n",
    "#-------------------------------------------------------------------------\n",
    "def tokenize(text):\n",
    "    \"\"\"A simple space-based tokenizer.\"\"\"\n",
    "    return str(text).lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:05.854516Z",
     "iopub.status.busy": "2025-11-05T17:45:05.854257Z",
     "iopub.status.idle": "2025-11-05T17:45:05.858256Z",
     "shell.execute_reply": "2025-11-05T17:45:05.857642Z",
     "shell.execute_reply.started": "2025-11-05T17:45:05.854496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:07.035376Z",
     "iopub.status.busy": "2025-11-05T17:45:07.034772Z",
     "iopub.status.idle": "2025-11-05T17:45:07.059455Z",
     "shell.execute_reply": "2025-11-05T17:45:07.058763Z",
     "shell.execute_reply.started": "2025-11-05T17:45:07.035351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use counter to count all tokens in train_df\n",
    "token_counter = Counter()\n",
    "# ------------------- write your code here -------------------------------\n",
    "#------------------------------------------------------------------------\n",
    "for text in train_df['text']:\n",
    "    tokens = tokenize(text)\n",
    "    token_counter.update(tokens)\n",
    "#------------------------------------------------------------------------\n",
    "print(\"Vocabulary build complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:08.954038Z",
     "iopub.status.busy": "2025-11-05T17:45:08.953782Z",
     "iopub.status.idle": "2025-11-05T17:45:08.959634Z",
     "shell.execute_reply": "2025-11-05T17:45:08.959091Z",
     "shell.execute_reply.started": "2025-11-05T17:45:08.954020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#----------------------------- DON'T CHANGE THIS --------------------------\n",
    "specials = ['<unk>', '<pad>']\n",
    "min_freq = 2\n",
    "vocab_list = specials + [token for token, freq in token_counter.items() if freq >= min_freq]\n",
    "word2idx = {token: i for i, token in enumerate(vocab_list)}\n",
    "# --- CRUCIAL ADDITIONS to make the provided code work ---\n",
    "# The functions below depend on these values\n",
    "UNK_IDX = word2idx['<unk>']\n",
    "PAD_IDX = word2idx['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:10.719196Z",
     "iopub.status.busy": "2025-11-05T17:45:10.718911Z",
     "iopub.status.idle": "2025-11-05T17:45:10.723525Z",
     "shell.execute_reply": "2025-11-05T17:45:10.722808Z",
     "shell.execute_reply.started": "2025-11-05T17:45:10.719174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    \"\"\"Converts text to a list of indices using the word2idx dict.\"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    return [word2idx.get(token, UNK_IDX) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:12.054599Z",
     "iopub.status.busy": "2025-11-05T17:45:12.054363Z",
     "iopub.status.idle": "2025-11-05T17:45:12.068659Z",
     "shell.execute_reply": "2025-11-05T17:45:12.067942Z",
     "shell.execute_reply.started": "2025-11-05T17:45:12.054582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.texts = dataframe['text'].values\n",
    "        self.labels = dataframe[['anger', 'fear', 'joy', 'sadness', 'surprise']].values.astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _labels) in batch:\n",
    "        label_list.append(_labels)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)[:MAX_LEN]\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    if text_list.shape[1] < MAX_LEN:\n",
    "        pad_tensor = torch.full(\n",
    "            (text_list.shape[0], MAX_LEN - text_list.shape[1]),\n",
    "            PAD_IDX,\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "        text_list = torch.cat((text_list, pad_tensor), dim=1)\n",
    "\n",
    "    return text_list, label_list\n",
    "\n",
    "\n",
    "# Create train and val dataloaders\n",
    "# ------------------- write your code here -------------------------------\n",
    "#------------------------------------------------------------------------\n",
    "train_dataset = EmotionDataset(train_df)\n",
    "val_dataset = EmotionDataset(val_df)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "#------------------------------------------------------------------------\n",
    "print(\"DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:19:45.472556Z",
     "iopub.status.busy": "2025-11-05T17:19:45.471933Z",
     "iopub.status.idle": "2025-11-05T17:19:45.476877Z",
     "shell.execute_reply": "2025-11-05T17:19:45.476054Z",
     "shell.execute_reply.started": "2025-11-05T17:19:45.472528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get vocabulary info\n",
    "vocab_size = len(vocab_list)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Padding token index (PAD_IDX): {PAD_IDX}\")\n",
    "print(f\"Unknown token index (UNK_IDX): {UNK_IDX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:19:47.853199Z",
     "iopub.status.busy": "2025-11-05T17:19:47.852857Z",
     "iopub.status.idle": "2025-11-05T17:19:47.858756Z",
     "shell.execute_reply": "2025-11-05T17:19:47.857895Z",
     "shell.execute_reply.started": "2025-11-05T17:19:47.853174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "words_to_check = ['happy', 'alone', 'sad']\n",
    "for word in words_to_check:\n",
    "    idx = word2idx.get(word, UNK_IDX)\n",
    "    print(f\"Index of '{word}': {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:19:49.432354Z",
     "iopub.status.busy": "2025-11-05T17:19:49.432065Z",
     "iopub.status.idle": "2025-11-05T17:19:49.522023Z",
     "shell.execute_reply": "2025-11-05T17:19:49.521047Z",
     "shell.execute_reply.started": "2025-11-05T17:19:49.432333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define an Embedding layer\n",
    "embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=EMB_DIM, padding_idx=PAD_IDX)\n",
    "\n",
    "# Take one batch from the train_dataloader\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "text_batch, label_batch = sample_batch\n",
    "\n",
    "print(f\"\\nInput batch shape: {text_batch.shape}\")  # should be (BATCH_SIZE, MAX_LEN)\n",
    "\n",
    "# Pass through embedding layer\n",
    "embedded_output = embedding(text_batch)\n",
    "\n",
    "print(f\"Output shape of Embedding layer: {embedded_output.shape}\")  # should be (BATCH_SIZE, MAX_LEN, EMB_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:18:44.242535Z",
     "iopub.status.busy": "2025-11-05T16:18:44.242187Z",
     "iopub.status.idle": "2025-11-05T16:18:44.254238Z",
     "shell.execute_reply": "2025-11-05T16:18:44.252578Z",
     "shell.execute_reply.started": "2025-11-05T16:18:44.242512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lstm = torch.nn.LSTM(\n",
    "    input_size=EMB_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    num_layers=1,\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:18:58.756537Z",
     "iopub.status.busy": "2025-11-05T16:18:58.756220Z",
     "iopub.status.idle": "2025-11-05T16:18:58.766118Z",
     "shell.execute_reply": "2025-11-05T16:18:58.765173Z",
     "shell.execute_reply.started": "2025-11-05T16:18:58.756514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(train_dataloader))\n",
    "print(f\"Input batch shape: {text_batch.shape}\")  # (BATCH_SIZE, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:00.349406Z",
     "iopub.status.busy": "2025-11-05T16:19:00.347478Z",
     "iopub.status.idle": "2025-11-05T16:19:00.569242Z",
     "shell.execute_reply": "2025-11-05T16:19:00.568222Z",
     "shell.execute_reply.started": "2025-11-05T16:19:00.349354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedded = embedding(text_batch)\n",
    "print(f\"After Embedding layer: {embedded.shape}\")  # (BATCH_SIZE, MAX_LEN, EMB_DIM)\n",
    "\n",
    "# Step 5: Pass through LSTM\n",
    "output, (h_n, c_n) = lstm(embedded)\n",
    "\n",
    "# Step 6: Print all shapes\n",
    "print(f\"LSTM output shape: {output.shape}\")        # (BATCH_SIZE, MAX_LEN, HIDDEN_DIM)\n",
    "print(f\"Hidden state shape (h_n): {h_n.shape}\")    # (num_layers, BATCH_SIZE, HIDDEN_DIM)\n",
    "print(f\"Cell state shape (c_n): {c_n.shape}\")      # (num_layers, BATCH_SIZE, HIDDEN_DIM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:02.387434Z",
     "iopub.status.busy": "2025-11-05T16:19:02.386587Z",
     "iopub.status.idle": "2025-11-05T16:19:02.392080Z",
     "shell.execute_reply": "2025-11-05T16:19:02.390803Z",
     "shell.execute_reply.started": "2025-11-05T16:19:02.387408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Hidden state shape (h_n): {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:10.246916Z",
     "iopub.status.busy": "2025-11-05T16:19:10.246566Z",
     "iopub.status.idle": "2025-11-05T16:19:10.256748Z",
     "shell.execute_reply": "2025-11-05T16:19:10.255383Z",
     "shell.execute_reply.started": "2025-11-05T16:19:10.246892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(\n",
    "    input_size=EMB_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    num_layers=1,\n",
    "    batch_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:11.922810Z",
     "iopub.status.busy": "2025-11-05T16:19:11.922467Z",
     "iopub.status.idle": "2025-11-05T16:19:12.056432Z",
     "shell.execute_reply": "2025-11-05T16:19:12.054747Z",
     "shell.execute_reply.started": "2025-11-05T16:19:11.922786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH_SIZE, MAX_LEN, EMB_DIM)\n",
    "output, h_layer = gru(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:14.305017Z",
     "iopub.status.busy": "2025-11-05T16:19:14.304669Z",
     "iopub.status.idle": "2025-11-05T16:19:14.310912Z",
     "shell.execute_reply": "2025-11-05T16:19:14.309552Z",
     "shell.execute_reply.started": "2025-11-05T16:19:14.304994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Output shape of GRU:\", output.shape)\n",
    "print(\"Hidden state shape of GRU:\", h_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:15.686762Z",
     "iopub.status.busy": "2025-11-05T16:19:15.686425Z",
     "iopub.status.idle": "2025-11-05T16:19:15.704258Z",
     "shell.execute_reply": "2025-11-05T16:19:15.703353Z",
     "shell.execute_reply.started": "2025-11-05T16:19:15.686741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bidir_lstm = nn.LSTM(\n",
    "    input_size=EMB_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    num_layers=1,\n",
    "    batch_first=True,\n",
    "    bidirectional=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:17.448532Z",
     "iopub.status.busy": "2025-11-05T16:19:17.448098Z",
     "iopub.status.idle": "2025-11-05T16:19:17.636658Z",
     "shell.execute_reply": "2025-11-05T16:19:17.634072Z",
     "shell.execute_reply.started": "2025-11-05T16:19:17.448503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH_SIZE, MAX_LEN, EMB_DIM)\n",
    "\n",
    "# forward pass\n",
    "output, (h_n, c_n) = bidir_lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:18.914070Z",
     "iopub.status.busy": "2025-11-05T16:19:18.886552Z",
     "iopub.status.idle": "2025-11-05T16:19:18.919460Z",
     "shell.execute_reply": "2025-11-05T16:19:18.918431Z",
     "shell.execute_reply.started": "2025-11-05T16:19:18.914029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Hidden state shape:\", h_n.shape)\n",
    "print(\"Cell state shape:\", c_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:20.497632Z",
     "iopub.status.busy": "2025-11-05T16:19:20.497300Z",
     "iopub.status.idle": "2025-11-05T16:19:20.505640Z",
     "shell.execute_reply": "2025-11-05T16:19:20.504153Z",
     "shell.execute_reply.started": "2025-11-05T16:19:20.497612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(5730, EMB_DIM)\n",
    "        self.lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])  # take last hidden output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:23.061850Z",
     "iopub.status.busy": "2025-11-05T16:19:23.061507Z",
     "iopub.status.idle": "2025-11-05T16:19:23.077847Z",
     "shell.execute_reply": "2025-11-05T16:19:23.076745Z",
     "shell.execute_reply.started": "2025-11-05T16:19:23.061828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:24.632422Z",
     "iopub.status.busy": "2025-11-05T16:19:24.631974Z",
     "iopub.status.idle": "2025-11-05T16:19:24.638732Z",
     "shell.execute_reply": "2025-11-05T16:19:24.637250Z",
     "shell.execute_reply.started": "2025-11-05T16:19:24.632398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:26.261983Z",
     "iopub.status.busy": "2025-11-05T16:19:26.261639Z",
     "iopub.status.idle": "2025-11-05T16:19:26.268112Z",
     "shell.execute_reply": "2025-11-05T16:19:26.266581Z",
     "shell.execute_reply.started": "2025-11-05T16:19:26.261961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total Trainable Parameters in LSTM Model: {count_params(lstm_model):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:27.572138Z",
     "iopub.status.busy": "2025-11-05T16:19:27.571605Z",
     "iopub.status.idle": "2025-11-05T16:19:27.579655Z",
     "shell.execute_reply": "2025-11-05T16:19:27.578530Z",
     "shell.execute_reply.started": "2025-11-05T16:19:27.572113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(5730, EMB_DIM)\n",
    "        self.lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM * 2, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:29.776818Z",
     "iopub.status.busy": "2025-11-05T16:19:29.776267Z",
     "iopub.status.idle": "2025-11-05T16:19:29.797571Z",
     "shell.execute_reply": "2025-11-05T16:19:29.796600Z",
     "shell.execute_reply.started": "2025-11-05T16:19:29.776786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bilstm_model = BiLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:32.854819Z",
     "iopub.status.busy": "2025-11-05T16:19:32.853555Z",
     "iopub.status.idle": "2025-11-05T16:19:32.860024Z",
     "shell.execute_reply": "2025-11-05T16:19:32.858826Z",
     "shell.execute_reply.started": "2025-11-05T16:19:32.854789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total Trainable Parameters in BILSTM Model: {count_params(bilstm_model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:35.642373Z",
     "iopub.status.busy": "2025-11-05T16:19:35.641591Z",
     "iopub.status.idle": "2025-11-05T16:19:35.648567Z",
     "shell.execute_reply": "2025-11-05T16:19:35.647552Z",
     "shell.execute_reply.started": "2025-11-05T16:19:35.642336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(5730, EMB_DIM)\n",
    "        self.gru = nn.GRU(EMB_DIM, HIDDEN_DIM, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h_n = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:37.445851Z",
     "iopub.status.busy": "2025-11-05T16:19:37.444918Z",
     "iopub.status.idle": "2025-11-05T16:19:37.468336Z",
     "shell.execute_reply": "2025-11-05T16:19:37.467064Z",
     "shell.execute_reply.started": "2025-11-05T16:19:37.445822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stacked_GRU_model = StackedGRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:39.787044Z",
     "iopub.status.busy": "2025-11-05T16:19:39.786747Z",
     "iopub.status.idle": "2025-11-05T16:19:39.792740Z",
     "shell.execute_reply": "2025-11-05T16:19:39.791559Z",
     "shell.execute_reply.started": "2025-11-05T16:19:39.787023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total Trainable Parameters in Stacked GRU Model: {count_params(stacked_GRU_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:32.690108Z",
     "iopub.status.busy": "2025-11-05T17:45:32.689862Z",
     "iopub.status.idle": "2025-11-05T17:45:32.786618Z",
     "shell.execute_reply": "2025-11-05T17:45:32.785933Z",
     "shell.execute_reply.started": "2025-11-05T17:45:32.690091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:34.294369Z",
     "iopub.status.busy": "2025-11-05T17:45:34.294019Z",
     "iopub.status.idle": "2025-11-05T17:45:37.537916Z",
     "shell.execute_reply": "2025-11-05T17:45:37.537188Z",
     "shell.execute_reply.started": "2025-11-05T17:45:34.294343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:47.494069Z",
     "iopub.status.busy": "2025-11-05T16:19:47.493733Z",
     "iopub.status.idle": "2025-11-05T16:19:47.500241Z",
     "shell.execute_reply": "2025-11-05T16:19:47.499185Z",
     "shell.execute_reply.started": "2025-11-05T16:19:47.494035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:49.906851Z",
     "iopub.status.busy": "2025-11-05T16:19:49.906497Z",
     "iopub.status.idle": "2025-11-05T16:19:49.914010Z",
     "shell.execute_reply": "2025-11-05T16:19:49.912752Z",
     "shell.execute_reply.started": "2025-11-05T16:19:49.906827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:19:52.083121Z",
     "iopub.status.busy": "2025-11-05T16:19:52.082745Z",
     "iopub.status.idle": "2025-11-05T16:19:52.091454Z",
     "shell.execute_reply": "2025-11-05T16:19:52.089595Z",
     "shell.execute_reply.started": "2025-11-05T16:19:52.083094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h_n = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:40.244668Z",
     "iopub.status.busy": "2025-11-05T17:45:40.243967Z",
     "iopub.status.idle": "2025-11-05T17:45:40.252642Z",
     "shell.execute_reply": "2025-11-05T17:45:40.251972Z",
     "shell.execute_reply.started": "2025-11-05T17:45:40.244645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_name,model, train_dataloader, val_dataloader, epochs=5, lr=1e-3):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"ðŸš€Starting training of {model_name}ðŸš€\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        print(f\"\\nðŸŸ© Starting Epoch {epoch+1}/{epochs}...\")\n",
    "\n",
    "        for batch_idx, (text, labels) in enumerate(train_dataloader):\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ðŸ•µï¸ Debug prints (first batch only)\n",
    "            if batch_idx == 0:\n",
    "                print(\"Text shape:\", text.shape)\n",
    "                print(\"Label shape:\", labels.shape)\n",
    "                print(\"Text dtype:\", text.dtype)\n",
    "                print(\"Label dtype:\", labels.dtype)\n",
    "\n",
    "            outputs = model(text)\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                print(\"Output shape:\", outputs.shape)\n",
    "                print(\"Output dtype:\", outputs.dtype)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for text, labels in val_dataloader:\n",
    "                text, labels = text.to(device), labels.to(device)\n",
    "                outputs = model(text)\n",
    "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds_bin = (all_preds >= 0.5).astype(int)\n",
    "        macro_f1 = f1_score(all_labels, all_preds_bin, average='macro')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "    print(\"âœ… Training complete!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:25:18.730015Z",
     "iopub.status.busy": "2025-11-05T16:25:18.729661Z",
     "iopub.status.idle": "2025-11-05T16:25:18.744856Z",
     "shell.execute_reply": "2025-11-05T16:25:18.743984Z",
     "shell.execute_reply.started": "2025-11-05T16:25:18.729990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_lstm = LSTMModel(len(vocab_list)+1, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:25:29.145755Z",
     "iopub.status.busy": "2025-11-05T16:25:29.145431Z",
     "iopub.status.idle": "2025-11-05T16:27:43.518701Z",
     "shell.execute_reply": "2025-11-05T16:27:43.517803Z",
     "shell.execute_reply.started": "2025-11-05T16:25:29.145734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# trained_model = train_and_evaluate(\"LSTM Model\",model_lstm, train_dataloader, val_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:27:52.413156Z",
     "iopub.status.busy": "2025-11-05T16:27:52.411746Z",
     "iopub.status.idle": "2025-11-05T16:27:52.427928Z",
     "shell.execute_reply": "2025-11-05T16:27:52.426778Z",
     "shell.execute_reply.started": "2025-11-05T16:27:52.413119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_gru = GRUModel(len(vocab_list)+1, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:13:34.146519Z",
     "iopub.status.busy": "2025-11-05T17:13:34.145994Z",
     "iopub.status.idle": "2025-11-05T17:13:34.153127Z",
     "shell.execute_reply": "2025-11-05T17:13:34.151013Z",
     "shell.execute_reply.started": "2025-11-05T17:13:34.146494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# trained_model_2 = train_and_evaluate(\"GRU Model\",model_gru, train_dataloader, val_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:45:58.355005Z",
     "iopub.status.busy": "2025-11-05T17:45:58.354331Z",
     "iopub.status.idle": "2025-11-05T17:45:58.359722Z",
     "shell.execute_reply": "2025-11-05T17:45:58.359201Z",
     "shell.execute_reply.started": "2025-11-05T17:45:58.354984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, pad_idx, num_layers=5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        # num_layers=2 makes it a stacked GRU\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h_n = self.gru(x)\n",
    "        # out[:, -1, :] -> last time step of the top layer\n",
    "        return self.fc(out[:, -1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:46:01.448908Z",
     "iopub.status.busy": "2025-11-05T17:46:01.448660Z",
     "iopub.status.idle": "2025-11-05T17:46:01.820502Z",
     "shell.execute_reply": "2025-11-05T17:46:01.819680Z",
     "shell.execute_reply.started": "2025-11-05T17:46:01.448891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stacked_gru = StackedGRU(len(vocab_list)+1, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:46:06.419099Z",
     "iopub.status.busy": "2025-11-05T17:46:06.418551Z",
     "iopub.status.idle": "2025-11-05T17:55:52.541635Z",
     "shell.execute_reply": "2025-11-05T17:55:52.540842Z",
     "shell.execute_reply.started": "2025-11-05T17:46:06.419075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model_3 = train_and_evaluate(\"Stacked GRU Model\",stacked_gru, train_dataloader, val_dataloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:59:54.465803Z",
     "iopub.status.busy": "2025-11-05T16:59:54.465464Z",
     "iopub.status.idle": "2025-11-05T17:00:03.251910Z",
     "shell.execute_reply": "2025-11-05T17:00:03.251098Z",
     "shell.execute_reply.started": "2025-11-05T16:59:54.465783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Login to W&B using your API key\n",
    "wandb.login(key=\"96513a18cdfc4585db6e8a169369fcc713e8ef2c\")  # <-- yahan apna API key daal de\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(project=\"emotion_detection\", name=\"Logged_Existing_Model\")\n",
    "\n",
    "# Putting our model in eval mode\n",
    "\n",
    "# LSTM\n",
    "# model_lstm.eval()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_lstm.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# GRU\n",
    "# model_gru.eval()\n",
    "# model_gru.to(device)\n",
    "\n",
    "\n",
    "# Stacked_GRU\n",
    "stacked_gru.eval()\n",
    "stacked_gru.to(device)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "# Going through validation data to collect predictions\n",
    "with torch.no_grad():\n",
    "    for text, labels in val_dataloader:\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        outputs = stacked_gru(text)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Converting probabilities to binary predictions\n",
    "all_preds_bin = (all_preds >= 0.5).astype(int)\n",
    "\n",
    "# Computing macro F1 score\n",
    "macro_f1 = f1_score(all_labels, all_preds_bin, average=\"macro\")\n",
    "\n",
    "# Logging to W&B\n",
    "wandb.log({\n",
    "    \"val_macro_f1\": macro_f1\n",
    "})\n",
    "\n",
    "print(f\"âœ… Logged Macro F1 to W&B: {macro_f1:.4f}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:52.543589Z",
     "iopub.status.busy": "2025-11-05T17:55:52.543126Z",
     "iopub.status.idle": "2025-11-05T17:55:52.571421Z",
     "shell.execute_reply": "2025-11-05T17:55:52.570766Z",
     "shell.execute_reply.started": "2025-11-05T17:55:52.543571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_df\n",
    "test_df = pd.read_csv(\"../Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:52.572376Z",
     "iopub.status.busy": "2025-11-05T17:55:52.572127Z",
     "iopub.status.idle": "2025-11-05T17:55:52.576800Z",
     "shell.execute_reply": "2025-11-05T17:55:52.575976Z",
     "shell.execute_reply.started": "2025-11-05T17:55:52.572357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.texts = dataframe['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:52.578551Z",
     "iopub.status.busy": "2025-11-05T17:55:52.578367Z",
     "iopub.status.idle": "2025-11-05T17:55:52.594115Z",
     "shell.execute_reply": "2025-11-05T17:55:52.593461Z",
     "shell.execute_reply.started": "2025-11-05T17:55:52.578537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_test_batch(batch):\n",
    "    text_list = []\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)[:MAX_LEN]\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    if text_list.shape[1] < MAX_LEN:\n",
    "        pad_tensor = torch.full(\n",
    "            (text_list.shape[0], MAX_LEN - text_list.shape[1]),\n",
    "            PAD_IDX,\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "        text_list = torch.cat((text_list, pad_tensor), dim=1)\n",
    "    \n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:52.595076Z",
     "iopub.status.busy": "2025-11-05T17:55:52.594858Z",
     "iopub.status.idle": "2025-11-05T17:55:52.611895Z",
     "shell.execute_reply": "2025-11-05T17:55:52.611379Z",
     "shell.execute_reply.started": "2025-11-05T17:55:52.595057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_df)  # test_df me sirf text column hona chahiye\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_test_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:52.612686Z",
     "iopub.status.busy": "2025-11-05T17:55:52.612531Z",
     "iopub.status.idle": "2025-11-05T17:55:53.263065Z",
     "shell.execute_reply": "2025-11-05T17:55:53.262397Z",
     "shell.execute_reply.started": "2025-11-05T17:55:52.612674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = stacked_gru  # stacked GRU trained model\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in test_dataloader:\n",
    "        text = text.to(device)\n",
    "        outputs = model(text)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds_bin = (preds >= 0.5).astype(int)\n",
    "        all_preds.append(preds_bin)\n",
    "\n",
    "final_preds = np.concatenate(all_preds)\n",
    "print(final_preds.shape)  # (num_test_samples, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:53.264184Z",
     "iopub.status.busy": "2025-11-05T17:55:53.263899Z",
     "iopub.status.idle": "2025-11-05T17:55:53.270445Z",
     "shell.execute_reply": "2025-11-05T17:55:53.269844Z",
     "shell.execute_reply.started": "2025-11-05T17:55:53.264160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:53.271304Z",
     "iopub.status.busy": "2025-11-05T17:55:53.271081Z",
     "iopub.status.idle": "2025-11-05T17:55:53.285494Z",
     "shell.execute_reply": "2025-11-05T17:55:53.284964Z",
     "shell.execute_reply.started": "2025-11-05T17:55:53.271289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "# Create a DataFrame\n",
    "df_preds = pd.DataFrame(final_preds, columns=columns)\n",
    "\n",
    "# Optional: add an ID column\n",
    "df_preds.insert(0, 'id', range(0, len(df_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T17:55:53.286516Z",
     "iopub.status.busy": "2025-11-05T17:55:53.286269Z",
     "iopub.status.idle": "2025-11-05T17:55:53.312653Z",
     "shell.execute_reply": "2025-11-05T17:55:53.312118Z",
     "shell.execute_reply.started": "2025-11-05T17:55:53.286494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_preds.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Predictions saved to test_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13800781,
     "sourceId": 115439,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
